{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size=26000\n",
    "embeding_size=1024\n",
    "num_heads=12\n",
    "num_layers=24\n",
    "batch_size=1\n",
    "mid_size=1024\n",
    "max_length=10\n",
    "step_size=10000\n",
    "epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "  out = 1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3)))\n",
    "  return out * x / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_mask_matrix():\n",
    "  ans=torch.triu(torch.ones(batch_size,max_length,max_length),diagonal=1)\n",
    "  return ans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(get_mask_matrix())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(feedforward,self).__init__()\n",
    "    self.first_layer=nn.Linear(embeding_size,mid_size)\n",
    "    self.second_layer=nn.Linear(mid_size,embeding_size)\n",
    "    #self.activate=nn.Tanh()\n",
    "    self.layer_norm=nn.LayerNorm(embeding_size)\n",
    "  def forward(self,X):\n",
    "    mid_ans1=self.first_layer(X)\n",
    "    mid_ans2=gelu(mid_ans1)\n",
    "    mid_ans3=self.second_layer(mid_ans2)\n",
    "    mid_ans4=gelu(mid_ans3)\n",
    "    endans=self.layer_norm(mid_ans4)\n",
    "    return endans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Attention_layer,self).__init__()\n",
    "    self.W_Q=nn.Linear(embeding_size,embeding_size)\n",
    "    self.W_K=nn.Linear(embeding_size,embeding_size)\n",
    "    self.W_V=nn.Linear(embeding_size,embeding_size)\n",
    "    self.W_O=nn.Linear(embeding_size,embeding_size)\n",
    "  def forward(self,X):\n",
    "    Q=self.W_Q(X)\n",
    "    K=self.W_K(X)\n",
    "    V=self.W_V(X)\n",
    "    Q_heads=torch.split(Q,embeding_size//num_heads,dim=2)\n",
    "    K_heads=torch.split(K,embeding_size//num_heads,dim=2)\n",
    "    V_heads=torch.split(V,embeding_size//num_heads,dim=2)\n",
    "    A_matrix=[]\n",
    "    d_k=Q.size(-1)",
    "    Ans_matrix=[]\n",
    "    mask_matrix=get_mask_matrix()\n",
    "    mask_matrix1=mask_matrix.to(device)\n",
    "    for i in range(len(Q_heads)):\n",
    "      A_matrix.append(torch.bmm(Q_heads[i],torch.transpose(K_heads[i],1,2))+(-1e9)*mask_matrix1)\n",
    "    output=[]\n",
    "    for i in range(len(A_matrix)):\n",
    "      output.append(torch.bmm(nn.functional.softmax(A_matrix[i],dim=2),V_heads[i])/math.sqrt(d_k))\n",
    "    mid_output=torch.cat([x for x in output],2)\n",
    "    #print(mid_output)\n",
    "    endoutput=mid_output+self.W_O(X)\n",
    "    return endoutput"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(chineseGPT,self).__init__()\n",
    "    self.attention=nn.Sequential()\n",
    "    for i in range(num_layers):\n",
    "      self.attention.add_module(f\"attention{i}\",Attention_layer())\n",
    "      self.attention.add_module(f\"feedworward{i}\",feedforward())\n",
    "  def forward(self,X):\n",
    "    ans=self.attention(X)\n",
    "    return ans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(classifier,self).__init__()\n",
    "    self.first_layer=nn.Linear(embeding_size,mid_size)\n",
    "    self.second_layer=nn.Linear(mid_size,embeding_size)\n",
    "    self.third_layer=nn.Linear(embeding_size,vocab_size)\n",
    "    #self.activate=nn.Tanh()\n",
    "  def forward(self,X):\n",
    "    mid_ans1=self.first_layer(X)\n",
    "    mid_ans2=gelu(mid_ans1)\n",
    "    mid_ans3=self.second_layer(mid_ans2)\n",
    "    mid_ans4=gelu(mid_ans3)\n",
    "    mid_ans5=self.third_layer(mid_ans4)\n",
    "    #endans=nn.functional.softmax(mid_ans5,dim=2)\n",
    "    return mid_ans5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
