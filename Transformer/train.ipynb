{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm,trange\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from dataloader.dataloader import MyData\n",
    "from model.Decoder_Only import Decoder_Only\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,batch_size,lr,learning_de,de_step,train_dataset,model,scaler):\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=int(batch_size),shuffle=True,drop_last=True)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    with autocast():\n",
    "        for i in range(epoch):\n",
    "            print(\"now_learning_rate:\"+str(optimizer.param_groups[0][\"lr\"]))\n",
    "            with tqdm(total=len(train_dataloader)) as _tqdm: # 使用需要的参数对tqdm进行初始化   \n",
    "                for data in train_dataloader:\n",
    "                    tokenizer_output=model.use_tokenizer(data)\n",
    "                    prediction=model.get_ans(tokenizer_output['input_ids'].to(os.environ.get('device')))\n",
    "                \n",
    "                    ls=model.loss(prediction,tokenizer_output['input_ids'].to(os.environ.get('device')),tokenizer_output['attention_mask'].to(os.environ.get('device')))\n",
    "                    _tqdm.set_postfix(loss='{:.3f}'.format(ls)) # 设置你想要在本次循环内实时监视的变量  可以作为后缀打印出来\n",
    "                    _tqdm.update(1)\n",
    "                    scaler.scale(ls).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    optimizer.zero_grad() \n",
    "        torch.save(model.state_dict(), 'model')\n",
    "        if (i+1)%de_step==0:\n",
    "            optimizer.param_groups[0][\"lr\"]=optimizer.param_groups[0][\"lr\"]*learning_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /root/.cache/modelscope/hub/models/./OpenBMB/MiniCPM3-4B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:28:38,585 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "2025-02-27 23:28:38,966 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/tmp/ipykernel_42178/1219914656.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "vocab_size=73448\n",
    "epoch=1\n",
    "batch_size=16\n",
    "lr=1e-4\n",
    "de=0.5\n",
    "de_step=5\n",
    "os.environ['device']='cuda'\n",
    "\n",
    "\n",
    "num_layer=6\n",
    "num_header=12\n",
    "input_length=512\n",
    "embeding_dim=512\n",
    "use_group_attention=True\n",
    "group_num=3\n",
    "\n",
    "model=Decoder_Only(\n",
    "    num_header=num_header,\n",
    "    num_layer=num_layer,\n",
    "    embeding_dim=embeding_dim,\n",
    "    input_length=input_length,\n",
    "    vocab_size=vocab_size,\n",
    "    use_group_attention=use_group_attention,\n",
    "    group_num=group_num,\n",
    "    use_check_point=True\n",
    "    ).to(os.environ.get('device'))\n",
    "\n",
    "train_dataset=MyData()\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42178/78680193.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now_learning_rate:0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/62888 [00:00<15:26:01,  1.13it/s, loss=5.331]/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "  0%|          | 237/62888 [05:43<25:14:07,  1.45s/it, loss=3.064]"
     ]
    }
   ],
   "source": [
    "train(epoch,batch_size,lr,de,de_step,train_dataset,model,scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
